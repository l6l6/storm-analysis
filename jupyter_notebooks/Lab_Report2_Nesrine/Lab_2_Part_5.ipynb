{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating average 2D localization precision from experimental data\n",
    "\n",
    "This notebook explains an approach to estimate 2D localization fitting performance that can be used on experimental data. The approach is based on histogramming nearest neighbor distances in adjacent frames. In this example we're going to use `3D-DAOSTORM` algorithm to do the finding/fitting for simplicity, but this approach can be used with any finding/fitting algorithm.\n",
    "\n",
    "Reference:\n",
    "* [Endesfelder et al, Histochemistry and Cell Biology, 2014](https://doi.org/10.1007/s00418-014-1192-3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the directory\n",
    "Create an empty directory somewhere on your computer and tell Python to go to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy\n",
    "import os\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Functions\n",
    "\n",
    "These are what we'll use to fit for the average localization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_library.ia_utilities_c as iaUtilsC\n",
    "import storm_analysis.sa_library.sa_h5py as saH5Py\n",
    "\n",
    "def calcPOfD(hdf5_name, bin_size = 5.0, max_cnts = 10000, n_bins = 40, pixel_size = 100.0, start_frame = 0):\n",
    "    \"\"\"\n",
    "    Calculate the p(d) histogram.\n",
    "    \n",
    "    The default is nearest neighbor distances out to 200nm with a 5nm bin\n",
    "    size and total of 10000 counts (more or less) in the histogram.\n",
    "    \n",
    "    hdf5_name - The name of the HDF5 localizations file.\n",
    "    bin_size - Bin size in nanometers.\n",
    "    max_cnts - (Approximate) maximum number of events in the histogram\n",
    "    n_bins - The number of bins in the histogram.\n",
    "    pixel_size - The size of a pixel in nanometers.\n",
    "    start_frame - The first frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_x = numpy.arange(n_bins) * bin_size + 0.5 * bin_size\n",
    "    p_d = numpy.zeros(n_bins)\n",
    "\n",
    "    cnts = 0\n",
    "    l_x = None\n",
    "    l_y = None\n",
    "    last_fnum = -10\n",
    "    with saH5Py.SAH5Py(hdf5_name) as h5:\n",
    "        for fnum, locs in h5.localizationsIterator(fields = [\"x\", \"y\"]):\n",
    "        \n",
    "            # Skip first N frames:\n",
    "            if (fnum < start_frame):\n",
    "                continue\n",
    "        \n",
    "            # Check for empty frame\n",
    "            if bool(locs):\n",
    "            \n",
    "                # Check for sequential frames.\n",
    "                if (fnum == (last_fnum + 1)):\n",
    "                \n",
    "                    # Find nearest localizations in previous frame to \n",
    "                    # localizations in current frame.\n",
    "                    dist = iaUtilsC.peakToPeakDistAndIndex(locs[\"x\"], locs[\"y\"], l_x, l_y)[0]\n",
    "                \n",
    "                    # Convert distance in pixels to nanometers.\n",
    "                    dist = dist * pixel_size\n",
    "                \n",
    "                    # Convert to bin size.\n",
    "                    dist = dist/bin_size\n",
    "                \n",
    "                    for i in range(dist.size):\n",
    "                        index = int(round(dist[i]))\n",
    "                        if (index >= 0) and (index < p_d.size):\n",
    "                            p_d[index] += 1\n",
    "                            cnts += 1\n",
    "                        \n",
    "                    if (cnts > max_cnts):\n",
    "                        break\n",
    "                \n",
    "                l_x = locs[\"x\"]\n",
    "                l_y = locs[\"y\"]\n",
    "            \n",
    "            last_fnum = fnum\n",
    "            \n",
    "    return [p_x, p_d]\n",
    "\n",
    "# For single term p(d) function to data.\n",
    "def fitPOfDDist(x, y, p0):\n",
    "    [coeff, var_matrix] = scipy.optimize.curve_fit(pOfDDist, x, y, p0=p0)\n",
    "    return [coeff, pOfDDist(x, *coeff)]\n",
    "\n",
    "# Fit corrected p(d) function to data.\n",
    "def fitPOfDDistCor(x, y, p0):\n",
    "    [coeff, var_matrix] = scipy.optimize.curve_fit(pOfDDistCor, x, y, p0=p0)\n",
    "    return [coeff, pOfDDistCor(x, *coeff)]\n",
    "\n",
    "# Single term p(d) function\n",
    "def pOfDDist(x, *p):\n",
    "    A1, sig_smlm = p\n",
    "\n",
    "    t1 = A1 * x/(2 * sig_smlm * sig_smlm) * numpy.exp(-x*x/(4.0 * sig_smlm * sig_smlm))\n",
    "    return t1\n",
    "\n",
    "# Corrected p(d) function\n",
    "def pOfDDistCor(x, *p):\n",
    "    A1, A2, A3, sig_smlm, w, dc = p\n",
    "    dd = x - dc\n",
    "\n",
    "    t1 = A1 * x/(2 * sig_smlm * sig_smlm) * numpy.exp(-x*x/(4.0 * sig_smlm * sig_smlm))\n",
    "    t2 = A2 * 1.0/numpy.sqrt(2.0 * numpy.pi * w * w) * numpy.exp(-dd*dd/(2.0*w*w))\n",
    "    t3 = A3 * x\n",
    "    return t1 + t2 + t3\n",
    "\n",
    "# Pretty print the corrected p(d) coefficients.\n",
    "def prettyPrintCoeff(coeff):\n",
    "    print(\"     A1: {0:.1f}\".format(coeff[0]))\n",
    "    print(\"     A2: {0:.1f}\".format(coeff[1]))    \n",
    "    print(\"     A3: {0:.2f}\".format(coeff[2]))\n",
    "    print(\"  sigma: {0:.2f}\".format(coeff[3]))\n",
    "    print(\"      w: {0:.2f}\".format(coeff[4]))\n",
    "    print(\"     dc: {0:.2f}\".format(coeff[5]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizations on a grid example\n",
    "\n",
    "Create a movie of localizations on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.jupyter_examples.est_fit_prec as estFitPrec\n",
    "\n",
    "# Set background, signal\n",
    "estFitPrec.bg = 20\n",
    "estFitPrec.signal = 2000\n",
    "\n",
    "# Make parameters file for analysis\n",
    "estFitPrec.createParametersFile()\n",
    "\n",
    "# Create ground truth localizations file\n",
    "estFitPrec.createLocalizationsGrid()\n",
    "\n",
    "# Create movie\n",
    "estFitPrec.createMovieGrid(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.daostorm_3d.mufit_analysis as mfit\n",
    "\n",
    "# Remove stale results, if any.\n",
    "if os.path.exists(\"grid_test.hdf5\"):\n",
    "    os.remove(\"grid_test.hdf5\")\n",
    "    \n",
    "# (Re)run the analysis.\n",
    "mfit.analyze(\"grid.tif\", \"grid_test.hdf5\", \"dao3d_analysis.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p(d) histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p_x, p_d] = calcPOfD(\"grid_test.hdf5\", bin_size = 2.0, pixel_size = estFitPrec.pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit uncorrected p(d) and compare to the error estimated from the ground truth locations and to the Cramer-Rao estimate.\n",
    "\n",
    "In this example this is all that is necessary as all the localizations were on and should be detected in every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_utilities.finding_fitting_error as ffe\n",
    "import storm_analysis.sa_utilities.mortensen as mortensen\n",
    "\n",
    "[coeff, fit] = fitPOfDDist(p_x, p_d, [numpy.max(p_d), 5.0])\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "# Ground truth comparison estimate.\n",
    "[dx, dy, dz] = ffe.findingFittingErrorHDF5File(\"grid_ref.hdf5\", \"grid_test.hdf5\")\n",
    "gt_sigma = 0.5*(numpy.std(dx) + numpy.std(dy))\n",
    "\n",
    "# Cramer-Rao bound.\n",
    "cr_sigma = mortensen.cramerRaoBound(estFitPrec.signal, \n",
    "                                    estFitPrec.bg, \n",
    "                                    estFitPrec.pixel_size, \n",
    "                                    estFitPrec.pixel_size * 1.5)\n",
    "\n",
    "print(\"Average precision estimate: {0:.2f}nm\".format(coeff[1]))\n",
    "print(\"Ground-truth comparison: {0:.2f}nm\".format(gt_sigma))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly distributed localizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth localizations file\n",
    "estFitPrec.createLocalizationsRandom()\n",
    "\n",
    "# Create movie, 250 frames is enough to reach the default max_cnts argument for calcPOfD().\n",
    "estFitPrec.createMovieRandom(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stale results, if any.\n",
    "if os.path.exists(\"random_test.hdf5\"):\n",
    "    os.remove(\"random_test.hdf5\")\n",
    "    \n",
    "# (Re)run the analysis.\n",
    "mfit.analyze(\"random.tif\", \"random_test.hdf5\", \"dao3d_analysis.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p(d) histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p_x, p_d] = calcPOfD(\"random_test.hdf5\", bin_size = 2.0, pixel_size = estFitPrec.pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit uncorrected p(d) to get initial estimates for some of the terms. \n",
    "\n",
    "Note that this deviates significantly from the actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[coeff, fit] = fitPOfDDist(p_x, p_d, [numpy.max(p_d), 5.0])\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "print(\"Average precision estimate: {0:.2f}nm\".format(coeff[1]))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit corrected p(d).\n",
    "\n",
    "This should match the actual distribution much better. Note that this fit is not perfectly stable so in practice you may have to adjust the starting parameters.\n",
    "\n",
    "The localization precision is now significantly worse than the Cramer-Rao bound. There are several reasons for this including overlapping localizations and localizations that are only on for part of frame. In the simulation the localization on times are exponentially distributed with an average on time of a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = [coeff[0], 0.1 * coeff[0], 0.0, coeff[1], coeff[1], 3 * coeff[1]]\n",
    "print(\"Initial guess:\")\n",
    "prettyPrintCoeff(x_o)\n",
    "print()\n",
    "\n",
    "[coeff_cor, fit_cor] = fitPOfDDistCor(p_x, p_d, x_o)\n",
    "print(\"After fitting:\")\n",
    "prettyPrintCoeff(coeff_cor)\n",
    "print()\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit_cor, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "print(\"Corrected average precision estimate: {0:.2f}nm\".format(coeff_cor[3]))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: General questions\n",
    "\n",
    "**4.  List 4 or more differences between the images you created and the neuronal data analyzed in part 1. Comment on the size, shapes of the PSF, differences in how dense the spacing is between spots, etc.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**5. Our ability to resolve structures in an image not only requires many localizations at high precision but also depends on the density of measured emitters.**\n",
    "\n",
    "\n",
    "**a. Explain why density would matter here. Please relate your answer to Nyquist sampling.**\n",
    "\n",
    "As per the Nyquist sampling theory, we require approximately two data points per resolution unit. Thus, for a two-dimensional image having spatial features of size $\\alpha$, the minimum required molecular density of localized  probes necessary to meet the Nyquist criterion is\n",
    "\\begin{equation}\n",
    "\\text{Nyquist Molecular Density} \\approx \\Big(\\frac{2}{\\alpha}\\Big)^2\n",
    "\\end{equation}\n",
    "\n",
    "**b. Sketch a sample/describe an experiment that would allow you to determine the resolution of your reconstruction. Give a short description of your illustration.**\n",
    "\n",
    "\n",
    " The Abbe limit of resolution for optical images is approximately 0.22 micrometers (using visible light), meaning that a digitizer must be capable of sampling at intervals that correspond in the specimen space to 0.11 micrometers or less. A digitizer that samples the specimen at 512 pixels per horizontal scan line would have to produce a maximum horizontal field of view of 56 micrometers (512 x 0.11 micrometers) in order to conform to the Nyquist criterion. An interval of 2.5 to 3 samples for the smallest resolvable feature is suggested to ensure adequate sampling for high resolution imaging. ([Reference here](https://zeiss-campus.magnet.fsu.edu/articles/basics/digitalimaging.html))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6. Would super-resolution imaging would be useful for measuring the cellular component you presented in class. Explain your answer (3-6 sentences).**\n",
    "\n",
    "Specific tools to visualize ECM components are essential. With optical microscopy, it is possible to better understand the spatial, temporal, and molecular changes that occur in the ECM, as well as cell adhesion points, cytoskeleton morphology, and other features of such a complex microenvironment. Recent developments and applications of super resolution imaging have been used to address increasingly complex biological questions related to ECM remodeling.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}