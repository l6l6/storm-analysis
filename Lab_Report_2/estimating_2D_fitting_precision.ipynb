{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating average 2D localization precision from experimental data\n",
    "\n",
    "This notebook explains an approach to estimate 2D localization fitting performance that can be used on experimental data. The approach is based on histogramming nearest neighbor distances in adjacent frames. In this example we're going to use `3D-DAOSTORM` algorithm to do the finding/fitting for simplicity, but this approach can be used with any finding/fitting algorithm.\n",
    "\n",
    "Reference:\n",
    "* [Endesfelder et al, Histochemistry and Cell Biology, 2014](https://doi.org/10.1007/s00418-014-1192-3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the directory\n",
    "Create an empty directory somewhere on your computer and tell Python to go to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy\n",
    "import os\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "os.chdir(\"/home/hbabcock/Data/storm_analysis/jy_testing/\")\n",
    "print(os.getcwd())\n",
    "\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Functions\n",
    "\n",
    "These are what we'll use to fit for the average localization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_library.ia_utilities_c as iaUtilsC\n",
    "import storm_analysis.sa_library.sa_h5py as saH5Py\n",
    "\n",
    "def calcPOfD(hdf5_name, bin_size = 5.0, max_cnts = 10000, n_bins = 40, pixel_size = 100.0, start_frame = 0):\n",
    "    \"\"\"\n",
    "    Calculate the p(d) histogram.\n",
    "    \n",
    "    The default is nearest neighbor distances out to 200nm with a 5nm bin\n",
    "    size and total of 10000 counts (more or less) in the histogram.\n",
    "    \n",
    "    hdf5_name - The name of the HDF5 localizations file.\n",
    "    bin_size - Bin size in nanometers.\n",
    "    max_cnts - (Approximate) maximum number of events in the histogram\n",
    "    n_bins - The number of bins in the histogram.\n",
    "    pixel_size - The size of a pixel in nanometers.\n",
    "    start_frame - The first frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_x = numpy.arange(n_bins) * bin_size + 0.5 * bin_size\n",
    "    p_d = numpy.zeros(n_bins)\n",
    "\n",
    "    cnts = 0\n",
    "    l_x = None\n",
    "    l_y = None\n",
    "    last_fnum = -10\n",
    "    with saH5Py.SAH5Py(hdf5_name) as h5:\n",
    "        for fnum, locs in h5.localizationsIterator(fields = [\"x\", \"y\"]):\n",
    "        \n",
    "            # Skip first N frames:\n",
    "            if (fnum < start_frame):\n",
    "                continue\n",
    "        \n",
    "            # Check for empty frame\n",
    "            if bool(locs):\n",
    "            \n",
    "                # Check for sequential frames.\n",
    "                if (fnum == (last_fnum + 1)):\n",
    "                \n",
    "                    # Find nearest localizations in previous frame to \n",
    "                    # localizations in current frame.\n",
    "                    dist = iaUtilsC.peakToPeakDistAndIndex(locs[\"x\"], locs[\"y\"], l_x, l_y)[0]\n",
    "                \n",
    "                    # Convert distance in pixels to nanometers.\n",
    "                    dist = dist * pixel_size\n",
    "                \n",
    "                    # Convert to bin size.\n",
    "                    dist = dist/bin_size\n",
    "                \n",
    "                    for i in range(dist.size):\n",
    "                        index = int(round(dist[i]))\n",
    "                        if (index >= 0) and (index < p_d.size):\n",
    "                            p_d[index] += 1\n",
    "                            cnts += 1\n",
    "                        \n",
    "                    if (cnts > max_cnts):\n",
    "                        break\n",
    "                \n",
    "                l_x = locs[\"x\"]\n",
    "                l_y = locs[\"y\"]\n",
    "            \n",
    "            last_fnum = fnum\n",
    "            \n",
    "    return [p_x, p_d]\n",
    "\n",
    "# For single term p(d) function to data.\n",
    "def fitPOfDDist(x, y, p0):\n",
    "    [coeff, var_matrix] = scipy.optimize.curve_fit(pOfDDist, x, y, p0=p0)\n",
    "    return [coeff, pOfDDist(x, *coeff)]\n",
    "\n",
    "# Fit corrected p(d) function to data.\n",
    "def fitPOfDDistCor(x, y, p0):\n",
    "    [coeff, var_matrix] = scipy.optimize.curve_fit(pOfDDistCor, x, y, p0=p0)\n",
    "    return [coeff, pOfDDistCor(x, *coeff)]\n",
    "\n",
    "# Single term p(d) function\n",
    "def pOfDDist(x, *p):\n",
    "    A1, sig_smlm = p\n",
    "\n",
    "    t1 = A1 * x/(2 * sig_smlm * sig_smlm) * numpy.exp(-x*x/(4.0 * sig_smlm * sig_smlm))\n",
    "    return t1\n",
    "\n",
    "# Corrected p(d) function\n",
    "def pOfDDistCor(x, *p):\n",
    "    A1, A2, A3, sig_smlm, w, dc = p\n",
    "    dd = x - dc\n",
    "\n",
    "    t1 = A1 * x/(2 * sig_smlm * sig_smlm) * numpy.exp(-x*x/(4.0 * sig_smlm * sig_smlm))\n",
    "    t2 = A2 * 1.0/numpy.sqrt(2.0 * numpy.pi * w * w) * numpy.exp(-dd*dd/(2.0*w*w))\n",
    "    t3 = A3 * x\n",
    "    return t1 + t2 + t3\n",
    "\n",
    "# Pretty print the corrected p(d) coefficients.\n",
    "def prettyPrintCoeff(coeff):\n",
    "    print(\"     A1: {0:.1f}\".format(coeff[0]))\n",
    "    print(\"     A2: {0:.1f}\".format(coeff[1]))    \n",
    "    print(\"     A3: {0:.2f}\".format(coeff[2]))\n",
    "    print(\"  sigma: {0:.2f}\".format(coeff[3]))\n",
    "    print(\"      w: {0:.2f}\".format(coeff[4]))\n",
    "    print(\"     dc: {0:.2f}\".format(coeff[5]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizations on a grid example\n",
    "\n",
    "Create a movie of localizations on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.jupyter_examples.est_fit_prec as estFitPrec\n",
    "\n",
    "# Set background, signal\n",
    "estFitPrec.bg = 20\n",
    "estFitPrec.signal = 2000\n",
    "\n",
    "# Make parameters file for analysis\n",
    "estFitPrec.createParametersFile()\n",
    "\n",
    "# Create ground truth localizations file\n",
    "estFitPrec.createLocalizationsGrid()\n",
    "\n",
    "# Create movie\n",
    "estFitPrec.createMovieGrid(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.daostorm_3d.mufit_analysis as mfit\n",
    "\n",
    "# Remove stale results, if any.\n",
    "if os.path.exists(\"grid_test.hdf5\"):\n",
    "    os.remove(\"grid_test.hdf5\")\n",
    "    \n",
    "# (Re)run the analysis.\n",
    "mfit.analyze(\"grid.tif\", \"grid_test.hdf5\", \"dao3d_analysis.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p(d) histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p_x, p_d] = calcPOfD(\"grid_test.hdf5\", bin_size = 2.0, pixel_size = estFitPrec.pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit uncorrected p(d) and compare to the error estimated from the ground truth locations and to the Cramer-Rao estimate.\n",
    "\n",
    "In this example this is all that is necessary as all the localizations were on and should be detected in every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_utilities.finding_fitting_error as ffe\n",
    "import storm_analysis.sa_utilities.mortensen as mortensen\n",
    "\n",
    "[coeff, fit] = fitPOfDDist(p_x, p_d, [numpy.max(p_d), 5.0])\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "# Ground truth comparison estimate.\n",
    "[dx, dy, dz] = ffe.findingFittingErrorHDF5File(\"grid_ref.hdf5\", \"grid_test.hdf5\")\n",
    "gt_sigma = 0.5*(numpy.std(dx) + numpy.std(dy))\n",
    "\n",
    "# Cramer-Rao bound.\n",
    "cr_sigma = mortensen.cramerRaoBound(estFitPrec.signal, \n",
    "                                    estFitPrec.bg, \n",
    "                                    estFitPrec.pixel_size, \n",
    "                                    estFitPrec.pixel_size * 1.5)\n",
    "\n",
    "print(\"Average precision estimate: {0:.2f}nm\".format(coeff[1]))\n",
    "print(\"Ground-truth comparison: {0:.2f}nm\".format(gt_sigma))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly distributed localizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth localizations file\n",
    "estFitPrec.createLocalizationsRandom()\n",
    "\n",
    "# Create movie, 250 frames is enough to reach the default max_cnts argument for calcPOfD().\n",
    "estFitPrec.createMovieRandom(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stale results, if any.\n",
    "if os.path.exists(\"random_test.hdf5\"):\n",
    "    os.remove(\"random_test.hdf5\")\n",
    "    \n",
    "# (Re)run the analysis.\n",
    "mfit.analyze(\"random.tif\", \"random_test.hdf5\", \"dao3d_analysis.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p(d) histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p_x, p_d] = calcPOfD(\"random_test.hdf5\", bin_size = 2.0, pixel_size = estFitPrec.pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit uncorrected p(d) to get initial estimates for some of the terms. \n",
    "\n",
    "Note that this deviates significantly from the actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[coeff, fit] = fitPOfDDist(p_x, p_d, [numpy.max(p_d), 5.0])\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "print(\"Average precision estimate: {0:.2f}nm\".format(coeff[1]))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit corrected p(d).\n",
    "\n",
    "This should match the actual distribution much better. Note that this fit is not perfectly stable so in practice you may have to adjust the starting parameters.\n",
    "\n",
    "The localization precision is now significantly worse than the Cramer-Rao bound. There are several reasons for this including overlapping localizations and localizations that are only on for part of frame. In the simulation the localization on times are exponentially distributed with an average on time of a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = [coeff[0], 0.1 * coeff[0], 0.0, coeff[1], coeff[1], 3 * coeff[1]]\n",
    "print(\"Initial guess:\")\n",
    "prettyPrintCoeff(x_o)\n",
    "print()\n",
    "\n",
    "[coeff_cor, fit_cor] = fitPOfDDistCor(p_x, p_d, x_o)\n",
    "print(\"After fitting:\")\n",
    "prettyPrintCoeff(coeff_cor)\n",
    "print()\n",
    "\n",
    "pyplot.bar(p_x, p_d, 1.5)\n",
    "pyplot.plot(p_x, fit_cor, color = \"black\")\n",
    "pyplot.show()\n",
    "\n",
    "print(\"Corrected average precision estimate: {0:.2f}nm\".format(coeff_cor[3]))\n",
    "print(\"Cramer-Rao bound: {0:.2f}nm\".format(cr_sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
